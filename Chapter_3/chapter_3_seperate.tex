\def\be{\begin{equation}}
\def\ee{\end{equation}}

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{notoccite}
\usepackage{epigraph} % epigraph
\usepackage{float}

\usepackage[square, numbers, comma, sort&compress]{natbib}  % Use the "Natbib" style for the references in the Bibliography
\usepackage{verbatim,listings}  % Needed for the "comment" environment to make LaTeX comments
\usepackage{array}  % Needed for the "comment" environment to make LaTeX comments
\usepackage{../vector}  % Allows "\bvec{}" and "\buvec{}" for "blackboard" style bold vectors in maths

% \documentclass[a4paper,12pt]{article}
%\usepackage[a4paper,vmargin={20mm,20mm},hmargin={20mm,20mm}]{geometry}
\usepackage{amsmath,amsfonts,amsthm,color,psfrag,epsf,graphicx}
% \usepackage{pstricks}
\usepackage{enumerate,caption}
%\usepackage[lined,algonl,boxed]{algorithm2e}
\usepackage[ruled,linesnumbered,vlined]{../algorithm2e}
\usepackage{float}
% \SpecialCoor
\def\subsum{\mathit{\Sigma}}




\def\ifthesis{\iftrue}
\setcounter{secnumdepth}{2}
\newenvironment{myindentpar}[1]%
{\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
}
{\end{list}}




\graphicspath{{Figures/}}  % Location of the graphics files (set up for graphics to be in PDF format)
\usepackage{epigraph} % epigraph
%customize: \setlength{\epigraphwidth}{7cm}\setlength{\epigraphrule}{0pt}
%use: \epigraph{text}{reference}

%customize: \setlength{\epigraphwidth}{7cm}\setlength{\epigraphrule}{0pt}
%use: \epigraph{text}{reference}
\begin{document}
%\bibliographystyle{iopart-num}
\bibliographystyle{unsrt}
%\chapter{Particle In cell} % Write in your own chapter title
%label{Chapter3}
%\lhead{Chapter 3. \emph{A Chapter}} % Write in your own chapter title to set the page header
%In some cases simulations can provide a more realistic picture of a physical system than can be carried out theoretically and so can aid in the interpretation of experimental results

Add this to references
http://onlinelibrary.wiley.com/doi/10.1002/ctpp.200710072/full


Computer simulations are the third tool, alongside theory and experiment, that scientists can use to understand natural phenomena. For systems with many degrees of freedom such as a laboratory plasma with more than $10^{20}$ particles, simulations provide a means in which to model the system and develop an understanding of the physics. In the limit of numerical noise, simulations provide the user with perfect diagnostics, the ability to make measurements of desired quantities without disturbing the system. This is an invaluable asset for testing theoretical predictions. Simulations are therefore a powerful utensil that compliment experiments and theory. Deep insight into the physics of plasmas has been gained by the use of computer simulations. In 1964 Landau damping of electrostatic waves was observed in a computational plasma experiment by Dawson \cite{Dawson}. This phenomena had been predicted by theory but had no empirical observations to support it at the time. There have been huge increases in computational power since then and as that power has increased so has the capability of computer simulations to explore the natural world. Today a whole host of plasma simulation codes are used to study tokamak plasmas. The application of these codes range from simulating large volumes of a tokamak, providing macroscopic quantities such as plasma density and temperature to tracking individual particles in a small sample of the divertor region to determine their behaviour as they approach the plasma-surface boundary. Simulations can enhance theoretical understanding and provide measurements that would be impossible to make in a physical experiment.    

 %gyrokinetic codes are routinely used to model turbulance and instabilities in tokamak plasmas reference, 

%helping to guide theoretical understanding and experimental measurements.


In the ideal case of unlimited computing power and memory, simulation codes would model plasmas by following the trajectories of every particle in the plasma as they move due to self-consistent electric and magnetic fields. Each particle in the system would interact with every other particle via the electric field.  The simulations would be advanced in infinitesimally small time steps. At each time step the force acting on each particle due to every other particle would be calculated, this force would then go into Newton's equations of motion to supply each particle with a new velocity and position. Interaction forces would then need to be re-calculated and the cycle repeated until the simulation had run for the desired time. Even with the power of modern computers it is not possible to do this. A complete simulation of a  tokamak plasma would be required to follow $\approx 10^{21}$ particles for millions of time steps, calculating the force at each time step. The amount of operations required to calculate these particle-particle interactions for $n$ particles in the system scales on the order $n^2$. No computer is capable of handling this many particles. However it is still possible to capture all of the relevant physics without such stringent computing demands.

 
Various computational techniques have been developed to simulate plasmas and they broadly fall into one of three categories. Particle-In-Cell (PIC) codes which follow the trajectories of individual particles in the plasma, Fluid models that treat the electrons and ions as separate fluids and Hybrid models that use a combination of fluid and PIC techniques to model the plasma \cite{simulation_types}. Each method has certain advantages and disadvantages that determine where its use is applicable. These will be discussed below.

Out of the three categories PIC codes are considered to be the most fundamental way to model a plasma. Individual electrons and ions are tracked as they move across a spatial domain responding to self consistent electric and magnetic fields generated by the electric charge of the particles. The individual particles are in fact superparticles that represent many real particles.  Rather than particle-particle interactions between every pair of particles in the simulation, superparticles deposit charge at discrete grid points along the domain. Other field quantities such as the electrostatic potential and electric field are then calculated from this charge density. The field is then interpolated back to the particles from the grid points in order to generate a new velocity and position. The discretisation of the field values as well as the use of superparticles allows modelling of the plasma from first principles. The essential physics of a real plasma can be captured with far fewer particles than are present in a real experiment. However in order to reduce statistical noise in the simulations large numbers of superparticles must be followed and there are certain stability criteria that must be met. As a result PIC simulations are compute-intensive, the simulations take a long time to run and this restricts PIC simulations to studying small regions of plasma.

%Fluid codes ease the computational burden by treating the ions and electrons as separate fluids rather than individual particles. The plasma is described by the density, mean velocity and mean energy of the electrons and ions. This method implicitly assumes the particles are in equilibrium with a Maxwellian velocity distribution. The fluid equations are then solved to obtain the macroscopic quantities of the plasma. These equations require closure conditions that must be approximated. These assumptions hinder the accuracy of fluid modelling and limit its applicability. However fluid codes are much less demanding on computer resources and can model large areas of a tokamak.

Fluid codes ease the computational burden by treating the ions and electrons as separate fluids rather than individual particles. The plasma is described by the density, mean velocity and mean energy of the electrons and ions.  Fluid codes solve the fluid equations which arise by taking the moments of the governing kinetic equations. The fluid equations require closure conditions that must be approximated \cite{fluid_closure}. This method is applicable when the plasma is in thermal equilibrium \cite{sturrock1994plasma}. As fluid codes are less demanding on computer resources they can be used to study the evolution of large-scale instabilities in tokamak plasmas.


Hybrid models exist that combine aspects of fluid and PIC codes. A common implementation of this technique is to treat the electrons as a fluid while modelling the ions kinetically. In this case the electrons are a neutralising background with a Boltzmann velocity distribution. This technique increases the required time step  by orders of magnitude as only the motion of the ions has to be tracked rather than the faster motion of the electrons. This also reduces the number of particles that have to be followed.  As a result these models can be run in less time but are useful if the user is only interested in the ion dynamics. 
 % This enables the simulation to proceed much faster on ion time scales rather than having to resolve the individual motion of the electrons. This also reduces the number of particles that have to be followed. These models are useful if the user is only interested in the ion dynamics.


It is not possible to simulate a Langmuir probe without capturing the physics of the sheath \cite{probe_sims_fluid}. The plasma sheath cannot be correctly modelled by fluid codes as the particle distributions inside the sheath are far from being in equilibrium \cite{krek}. The exact position of the sheath boundary is not well defined either as there is a smooth transition from the pre-sheath to the sheath near surfaces. PIC codes on the other hand make no assumptions about the distribution of the particles, they allow for any distribution function in phase space. By applying the fundamental equations the PIC method is able to preserve most of the physics. An alternative method to PIC is to directly integrate the Vlasov equation, which in 1D is given by
\begin{equation}
\frac{\partial f_s}{\partial t} + v \frac{\partial f_s}{\partial x} + \frac{q_s E}{m_s} \frac{\partial f_s}{\partial v} = 0 
\end{equation}
where $f_s$ is the phase space distribution function for a given species $s$.
Direct Vlasov codes numerically solve the Vlasov equation on a phase space grid without the use of particles. The codes can be used for the same spatial regions and time scales as PIC codes but the algorithms are computationally expensive and suffer from numerical instability \cite{soton}. The benefits are that they do not suffer from numerical noise which effects PIC simulations due to the finite number of superparticles used \cite{vlasov}. Complex probe designs such as the Ball-pen probe, which will be discussed in chapter X, require the tracking of individual particles in order to interpret experimental data obtained by the probes. Therefore completely kinetic PIC simulations must be carried out. The rest of this chapter will proceed to describe the general PIC method before moving on to describe each step of the algorithm in more detail. 


%These simulations support theoretical and experimental studies and are used to understand experimental data. It is far easier to insert diagnostics into a computer simulation than it is to implement them into an experiment, a simulation therefore provides information that experiments are not able to do such as following the exact trajectory of a particle as it moves through space and time or keeping a track of its individual kinetic and potential energy. 



\subsection{General Method} 
%PIC codes were first used in 1955 for the simulation of hydrodynamic problems and have been used since the 1950's for the simulation of plasma systems \cite{Harlow}
%The PIC method was first developed by Harlow in 1955 to numerically solve the fluid dynamics of strongly contorting materials in two or three spatial dimensions \cite{Harlow}.


%" In the
%PIC scheme, particles are defined in continuum space in both position and velocity. Fields
%are defined at discrete locations in space. However, both fields and particles are defined at
%discrete times. Particle and field values are advanced sequentially in time, starting from initial
%conditions,"


%% They are intuitive  Their relative ease of implementation makes PIC codes a popular choice for kinetic modelling from first principles. PIC simulations track the position and velocity of all the particles in the simulation domain. Particles are given an initial position and velocity which is then updated at each time step based on the forces acting on the particle. One way to model a plasma would be to calculate the Coulomb forces acting on each particle due to every other particle in the system. The number of calculations required per time step in this particle-particle method scales as $n^2$ where $n$ is the number of particles. This scheme would not be viable for dense fusion plasmas.
 
 
It is not possible with modern day computers to simulate a plasma by tracking all $10^{21}$ particles and calculating all the particle-particle interactions for each pair of particles in the system. The PIC scheme overcomes this problem by using single particles to represent a given number of physical particles, so called superparticles and by introducing a spatial grid as shown in figure \ref{fig:grid}. The superparticles have the same charge to mass ratio as their real particle equivalents and so follow the same trajectories as that of a real particle. For the remainder of this chapter the word particle is synonymous with superparticle. The spatial domain of the simulation is discretised into grid cells. For simplicity equal length grid cells will be assumed but this is not a necessity for the PIC scheme. Varying grid sizes are often employed for computational efficiency, in order to resolve regions where steep gradients are expected to exist, and not over-resolve other regions. At these cell boundaries, field values, namely the charge density, electrostatic potential and electric field are calculated. However the particles see a continuous domain and are free to take any position within the simulation. The charge density at each grid point is determined by the locations of the particles. The other field values are calculated from this charge density. The mechanism in which particle positions across a continuous domain are converted to charge densities on a spatial grid and correspondingly the mechanism which translates field values from grid locations back to the individual particles is known as weighting and will be discussed in section \ref{Section:weight}.
At the beginning of an electrostatic PIC simulation the grid is loaded with a particular distribution of particles depending on the plasma parameters to be modelled. It then follows an algorithm as depicted in figure \ref{fig:piccycle}. Each particle deposits charge to neighbouring grid points, Poisson's equation is solved to obtain a potential, the derivative of this is used to calculate the electric field and the field is mapped back to the particles which are then accelerated and moved. Once the particles have been moved, the PIC algorithm is complete, time is advanced by one time step and the whole cycle restarts by calculation of a new charge density based on the updated particle positions. This cycle will carry on until a certain time has been reached or steady-state has been obtained. The aforementioned steps are essential to any application of the PIC method to plasma simulations. Additional steps can also be added to the cycle based on the requirements of the user. These steps can include Monte Carlo collisions between the particles, absorption and injection of particles and other boundary effects such as sputtering, secondary electron emission and specular reflection. These steps are often added to the end of the PIC cycle once all the original particles in the system have been moved. 
% and contribute to the charge density of their neighbouring grid cells. Various algorithms for charge deposition exist and will be discussed below. Once each particle has deposited charge to the grid the resulting charge density can be used to determine the electrostatic potential at each grid point by solving Poisson's equation. The derivative of this potential yields an electric field value at each grid point. These field values are then mapped back on to the particles often using the same algorithm as for the charge deposition. The field feeds into Newton's equations of motion in order to accelerate and move the particles. 


The introduction of the grid means particles interact with each other via a charge density rather than pair to pair interactions. For a simulation with $n$ particles the introduction of the grid reduces the amount of calculations required per time step to the order of $n$ rather than $n^2$ as in the particle-particle scheme. Splitting a physical, continuous domain  up into grid cells does have implications which need to be considered in order to ensure the simulation can still produce physically accurate results. The consequences of introducing a grid on to the domain and how this can still accurately represent a plasma are discussed in section \ref{Section:grid}. The grid allows the equations that determine field values and particle motion to be solved using finite differencing. A scheme that takes continuous differential equations and converts them so that they can be solved on discrete grids in space and time. The first step of finite differencing is to carry out a Taylor expansion. For a function $f(x)$, a discretised form of the first differential of the equation can be obtained as follows. First carry out a Taylor expansion in the forward direction
\be
f(x+\Delta x) = f(x) +\Delta x \frac{\partial f(x)}{\partial x} + \frac{\Delta x^2}{2 !}\frac{\partial ^2 f(x)}{\partial x^2} + \frac{\Delta x^3}{3!} \frac{\partial^3 f(x)}{\partial x^3}
\ee
Repeating the procedure in the backwards direction  
\be
f(x-\Delta x) = f(x) -\Delta x \frac{\partial f(x)}{\partial x} + \frac{\Delta x^2}{2 !}\frac{\partial ^2 f(x)}{\partial x^2} - \frac{\Delta x^3}{3!} \frac{\partial^3 f(x)}{\partial x^3}
\ee
The two equations can be combined giving 
\be
 \frac{\partial f(x)}{\partial x} = \frac{f(x+\Delta x)-f(x-\Delta x)}{2 \Delta x} +  \frac{\Delta x^3}{3!} \frac{\partial^3 f(x)}{\partial x^3}
\ee 
Combining the two equations in this way is known as central differencing. The term proportional to $\Delta x^3$ is dropped so this method is second order accurate.

 %The grid is a mathematical construct that makes it possible to solve differential equations such as Newtons equations of motion and Poissons equation by converting them into Finite Difference Equations (FDE).
%For simplicity a one dimensional model will be considered but PIC codes are used to model fully three dimensional problems. In the one dimensional case particles are free to move over a continuous domain of length $L$ and can take any position between 0 and $L$. Plasma particles interact with each other via direct collisions and electrostatic forces. To calculate the forces acting on each particle due to every other particle present in the system would be far too computationally expensive when there are large numbers of particles present such as the case in plasma simulations. Although this technique is commonly employed in condensed matter studies and is known as the particle-particle (PP) method. To make the simulations computationally  feasible PIC codes discretise the domain into a grid as shown below.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{grid}
\caption{A representation of a two dimensional grid. The particle (grey circle) moves through the domain and deposits charge on the grid points. The area of the rectangle is proportional to the amount of charge deposited at each grid point.}%\cite{grid}}
\label{fig:grid}
\end{figure}





Each step of the PIC cycle will now be detailed with VSim specific algorithms detailed where appropriate. % For each of these steps highly optimised algorithms have been developed that are suitable for specific users. For clarity the most general methods will be discussed with specific focus on the algorithms used by VSim. 

% The amount of real particles a superparticle represents is labelled the weight of the superparticle. Weights in PIC simulations can vary from $10^3 \rightarrow 10^24$ depending on the plasma to be modelled and the dimensionality of the simulation.

% The use of superparticles makes it feasible to simulate high density plasmas.

%A PIC simulation follows an algorithm that starts with weighting the charge density of particles on to each grid point, typically particles will contribute charge only to the grid points closest to them. Different weighting schemes are possible and will be discussed. Once the charge density has been calculated it is then used to work out the electric potential by solving Poisson's equation. Again the potential is only solved at each grid point. Potential values are now used to calculate electric field values at the grid points which are then interpolated back to the particles, typically using the same weighting as for charge density so that new particle velocities and positions can be determined. Once the particles have been moved, the PIC algorithm is complete, time is advanced by one time step and the whole cycle repeats with a new density being calculated based on the new positions of the particles. This cycle will carry on until a certain time has been reached or steady-state has been obtained. PIC simulations are restricted by the size of the grid cells and the value of the time step as will be explained in more detail.



\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{piccycle.pdf}
\caption{A flow chart of the essential steps of the PIC algorithm }
\label{fig:piccycle}
\end{figure}


%"A proper understanding of the sheath phenomena is also an important prerequisite for obtaining boundary conditions for fluid codes simulating tokamaks."
\subsubsection{Calculating the Charge Density} \label{Section:weight}
During this stage of the cycle, the charge density at each grid point is calculated from the individual particle locations. Weighting is the name given to the calculations which interpolate charge densities to the grid points from the continuous particle positions. Various weighting algorithms exist. The simplest weighting algorithm is to simply deposit all of a particles charge to its nearest grid point. This is the Nearest Grid Point (NGP) scheme. This is a zero order weighting method. Any particles within half a cell of a grid point are assigned to that grid point. Let $\Delta x$ be the cell width, x the distance between the particle and grid point $X$ and $W(X)$ denote the weighting at grid point $X$. In the NGP scheme 
 
%Weighting is the name given to the calculations which produce charge densities at the grid points from the continuous particle positions. Different weighting schemes have been developed and there is often a trade off between accuracy and computation time. The simplest weighting method is the Nearest Grid Point (NGP) scheme. This is a zero order weighting method in which the entirety of the particles charge is assigned to the grid point which it is closest to. Any particles within half a cell of a grid point are assigned to that grid point. Let the cell width be $\Delta x$, $x$ the distance from the grid point and $W(X)$ denote the weighting at grid point $X$. In the NGP scheme 

\be
    W(X)=
    \begin{cases}
      1, & \text{if}\ \left|x \right|  \leq \frac{\Delta x }{2}   \\
      0, & \text{otherwise}
    \end{cases}
\ee

This is a computationally fast weighting method as it only requires one grid point look up per particle but this comes at the expense of adding noise to the simulation. As a particle moves away from its original grid point and into the region of a new grid point the grid density at the new grid point suddenly jumps up to have a value of one and the grid point it just left falls down to zero. This weighting scheme is not commonly deployed due to the noisy transition as particles move between cells. It is possible to reduce this noise by spreading the charge of the particle over more grid points at the expense of increased computation time. First-order weighting also known as area weighting smooths the density and field fluctuations compared to NGP but is more computationally expensive as it requires two grid point look-ups for each particle. In this method, for a 1D simulation, each particle contributes charge to its nearest two grid points. The first step calculates the offset ($\delta$) of the particle from the closest grid point to its left. 
\be
\delta = x_i - X_j 
\ee 
where $x_i$ is the particles position and $X_j$ the x-coordinate of the grid point. $x_i > X_j$ always. The charge assigned to the $j^{th}$ grid point is then 
\be 
W(j) = 1-\delta
%q_j = q_c \left(1 - \delta\right)
\ee 
%where $q_c$ is the charge of the particle. 
The charge assigned to the j+1 cell is 
\be 
W(j+1) = \delta
%q_{j+1} = q_c \left(\delta \right)
\ee
Such that the total charge deposited to the grid equals the charge of the particle. This results in a much smoother contribution to the charge density as the particle propagates through the grid. This is the most commonly employed weighting scheme and is the default for VSim. Higher order weighting methods do exist such as quadratic and cubic splines, that are second order and third order accurate respectively. These schemes further smooth the non-physical noise at the expense of more computation time by increasing the number of grid points that a particle contributes its charge to. This does lead to complications at the edge of the simulations boundary where there are insufficient neighbouring grid points. 




%Use this for picutre %http://porl2.tripod.com/sitebuildercontent/sitebuilderfiles/mphysproject.pdf
\subsubsection{Calculating the Potential}
Now the charge density is known at each grid point, Poisson's equation for electrostatics can be solved to obtain the electrostatic potential.
\be
\nabla ^2 \psi  = -\frac{\rho}{\epsilon_0}
\ee
This can be solved numerically on a discretised grid using finite differencing.
%\be
%\psi(x+\Delta x) = \psi(x) + \Delta x \frac{\partial \psi}{\partial x} + \frac{{(\Delta x)}^2}{2}\frac{{\partial}^2\psi}{\partial x^2} + ...
%\label{eq:forward}
%\ee
%The same procedure can be applied in the backwards direction 
%\be
%\psi(x-\Delta x) = \psi(x) - \Delta x \frac{\partial \psi}{\partial x} + \frac{{(\Delta x)}^2}{2}\frac{{\partial}^2\psi}{\partial x^2} + ...
%\label{eq:backward}
%\ee
%These two equations can be combined to give an approximate value for the second derivative of potential. Summing \eqref{eq:forward} and \eqref{eq:backward} and rearranging gives 
For a 1D simulation Poisson's equation can be expressed in finite difference form
\be 
\frac{{\partial}^2\psi}{\partial x^2} = \frac{\psi(x+\Delta x) -2\psi(x) + \psi(x-\Delta x)}{{(\Delta x)}^2}  = - \frac{\rho_x}{\epsilon_0}
\label{eq:phisolver}
\ee 
%In FDM the solution to the equation is only known at the grid points, the potential is no longer a continuous function. Combining the forward difference and backward difference solution like this is known as central differencing and is second order accurate.
For clarity we rewrite \eqref{eq:phisolver} with labels based on the grid number $j$. 
\be 
\frac{{\partial}^2\psi}{\partial x^2} = \frac{\psi_{j+1} -2\psi_{j} + \psi_{j-1}}{{(\Delta x)}^2} = -\frac{\rho_j}{\epsilon_0}
\label{eq:phisolver1}
\ee 

The value of $\psi$ at grid point j, ($\psi_j$),  depends on the value of $\psi$ at the two grid points either side of it $(\psi_{j-1}$ and $\psi_{j+1})$, so the grid points are coupled together. In order to find the value of $\psi_j$ at $N$ different grid points requires the solution of $N$ coupled linear equations. These coupled equations can be expressed in matrix form. 
\be
\begin{pmatrix}
  B_{1} & C_{1}  \\
  A_{2} & B_{2} & C_2 \\
        & A_3  & B_3 & C_3   \\
        & & \ddots & \ddots & \ddots \\
        & & &  A_N & B_N
\end{pmatrix}
\begin{pmatrix} 
 \psi_1  \\ 
 \psi_2  \\ 
 \psi_3  \\ 
 \vdots  \\
 \psi_N
\end{pmatrix}
= 
\begin{pmatrix} 
 \rho_1  \\ 
 \rho_2  \\ 
 \rho_3  \\ 
 \vdots  \\
 \rho_N
\end{pmatrix}
\ee
Where $A=1, B=-2$ and $C=1$. A matrix like this with non-zero elements only on the diagonal and one place either side of it is known as a tri-diagonal matrix.  The value of charge density ($\rho_i$) at each grid point is known as it was calculated in the previous step of the PIC algorithm. This matrix equation must now be solved in order to obtain the potential at each grid point. There are various numerical methods to find the solution and they can be divided into two categories: iterative methods and direct methods. VSim supports both type of solvers. For parallel simulations running on multiple cores, the iterative solver is employed.
%  Iterative methods begin with an estimate for the solution and use this estimate to get a better estimate. The iterations continue until the error in the estimated solution is below a certain pre-set tolerance. Iterative solves are useful in PIC applications as the potential at the previous time step can be used as the initial estimate for the next time step \cite{poisson_solvers}. Direct methods only need to solve the equation once and do not rely on an initial estimate and so are generally faster in obtaining a solution. They are however harder to implement and more demanding on computer resources. ALSO WHY CAN'T BE USED FOR PARELLEL.
Before a solution can be found, the boundary conditions must be supplied as the grid points at the edge of the domain only have one neighbouring grid point. Two common choices for boundary conditions exist, Dirichlet boundary conditions where $\psi_1$ and $\psi_N$ are set to a fixed value or Neumann boundary conditions where the gradient of the potential is fixed at the boundary. 
The implementation of Dirichlet boundary conditions is simple, $B_1$ and $B_N$ are set equal to one, $C_1 =0$ and $A_N = 0$. $\psi_1$ and $\psi_N$ are then given the desired potential boundary values $\alpha $ and $\beta$  respectively. The first and last matrix equations then read
\be 
1.\psi_1 + 0.\psi_2 = \alpha 
\ee 
\be 
0.\psi_{N-1} + 1.\psi_N = \beta 
\ee 

Neumann boundary conditions involve fixing the gradient of the potential(i.e. the electric field) at the edge of the domain. This could be implemented by setting $B_1  = -\frac{1}{\Delta x}$, $C_1 = \frac{1}{\Delta x}$ and $\rho_1$ = $\alpha$. Thus giving the first line in the matrix equation as 
\be 
\frac{\psi_2 - \psi_1}{\Delta x} = \alpha 
\ee 

Once the boundary conditions have been supplied the tri-diagonal matrix equation can be solved.
 
%Plenty of tri-diagonal matrix solvers exist including the tridag solver found in Numerical Recipes\cite{NumericalRecipes}.
\subsubsection{Calculating the Electric Field}
%MENTION WHY YOU DON't CALCULATE FIELD STRAIGHT AWAY WITHOUT POTENTIAL
Once the potential is known at each grid point the electric field is easily found by calculating the gradient of the potential.
\be 
E = - \nabla \psi
\ee 
which in one dimension becomes
\be 
E = - \frac{\partial \psi}{\partial x}
\label{eq:electricfield}
\ee
This can be discretised as before using finite differencing.
\be
E_J = -\frac{\psi_{J+1}-\psi_{J}}{\Delta x}
\ee
%This can discretised as before using the central difference method. 
%\be 
%E_J = - \frac{\psi_{J+1} - \psi_{J-1}}{2 \Delta x}
%\ee 
%Central differencing is not applicable at the boundaries due to a lack of neighbouring grid points and so either the forward or backward difference method must be used which is only accurate to first order. This provides the following two conditions to calculate the electric field at the edge of the domain.
%\be
%E_0 = - \frac{\psi_{J+1}-\psi_0}{\Delta x}
%\ee 
%\be 
%E_N = - \frac{\psi_N - \psi_{N-1}}{\Delta x}
%\ee
%Using a first order equation at the boundaries reduces the accuracy throughout the solution to first order. Fortunately the accuracy can be improved to second order by carrying out a further Taylor expansion 
%\be
%\psi(x+2\Delta x) = \psi(x) + 2\Delta x \frac{\partial \psi}{\partial x} + \frac{{(2\Delta x)}^2}{2}\frac{{\partial}^2\psi}{\partial x^2} + ...
%\label{eq:forward2}
%\ee
%Combining equations \eqref{eq:forward} , \eqref{eq:forward2} and \eqref{eq:electricfield} gives 
%\be 
%E_0 = \frac{3\psi_0 + \psi_2 - 4\psi_1}{2\Delta x}
%\ee 
%The exact same method in the backwards direction gives 
%\be 
%E_N = \frac{4\psi_{N-1} - \psi_{N-2} - 3\psi_N}{2\Delta x}
%\ee
%The second order boundary conditions restore second order accuracy across the domain.
\subsubsection{The Particle Mover}
The final step in the PIC cycle is to calculate a new position and velocity for each particle in the simulation based on the forces acting on them. In order to do this the following equations of motion must be solved 
\be 
\vec{F}  = m \frac{d \vec{v}}{d t} = q(\vec{E} + \vec{v} \times \vec{B})
\label{eq:diff1}
\ee 
\be 
\vec{v} = \frac{d \vec{x}}{d t}
\label{eq:diff2}
\ee 
%
%For a particle in an electromagnetic field, the force experienced will be given by the Lorentz force
%\be
%\vec{F} = q\left[\vec{E} + \vec{v} x \vec{B}\right]
%\ee




%Ignoring the presence of a magnetic field for now and limiting to one dimension gives the following equation of motion.
%\be 
%\frac{d \vec{v(x)}}{d t} = \frac{q}{m} E(x)
%\label{eq:diff1}
%\ee

The particles positions and velocities can be found by integrating the differential equations \eqref{eq:diff1} and \eqref{eq:diff2} again using finite difference methods. VSim uses a leap-frog scheme with a Boris advance \cite{Boris} to push the particles. 
The leap-frog method involves offsetting the velocity by half a time step from the position. So the velocity of the particles is only known at half integer time steps while the positions are known at integer time steps. This requires the initial velocities of the particles to be moved back half a time step at the beginning of the simulation, a "de-acceleration", in order to have time centred velocities. This just requires calculating the fields as before. This method is known as Leap-frog because in order to calculate new positions requires a leap over the known velocity. The algorithm is demonstrated in figure \ref{fig:Leap-frog}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{leap_frog.pdf}
\caption{A graphical representation of the Leap-Frog scheme. Particle positions are known at integer time steps while velocities are known at half-integer time steps.}
\label{fig:Leap-frog}
\end{figure}
%Instead of using the velocity at time $t$ to move the particle from $t$ to $t+1$ like the Euler method, Leap frog uses the velocity at time $t+\frac{1}{2}$ i.e. the average velocity of the particle between those two times. This method is more accurate than Euler's method for the same computational expense. The only extra step involves pushing back the particles half a time step but this only needs to be carried out once. For this reason Leap-frog is always the preferred choice over Euler and it can be proven to be second order accurate \cite{second_order}.
In discretised form the equations of motion become
\be 
\frac{x_{t+1}-x_t}{\Delta_t} = v_{t+1/2}
\label{eq:new_pos}
\ee
\be
\frac{v_{t+1/2} - v_{t-1/2}}{\Delta t} = \frac{q}{m}\left[\vec{E_t} + \frac{(v_{t+1/2}+v_{t-1/2}) }{2} \times B_t \right]
\label{eq:new_vel}
\ee 
First equation \ref{eq:new_vel} must be solved to get the new velocity of the particle ($v_{t+1/2}$), this is then inserted into equation \ref{eq:new_pos} to obtain a new position for the particle ($x_{t+1}$). This is carried out for every particle in the system. 


The most common implementation of the Boris scheme separates the effects of the electric and magnetic fields.
Firstly half of the impulse due to the electric field is added to the particle's velocity creating an intermediate variable $v^-$
\be
v^- = v_{t-1/2} + \frac{q}{m} E_t \frac{\Delta t}{2}
\ee
The magnetic field then acts on $v^-$ to create a second intermediate variable $v^+$. The magnetic field only effects the rotation of the velocity vector not the magnitude.
\be 
\frac{v^+ - v^-}{\Delta t} = \frac{q}{2m} (v^+ + v^-) x B_t
\ee
Finally the second half of the electric impulse is added to $v^+$ to obtain the new velocity for the particle.
\be 
v_{t+1/2} =  v^+ + \frac{q}{m} E_t \frac{\Delta t}{2}
\label{eq:vnew}
\ee
The Boris scheme can be used to advance particles for an arbitrarily large number of time steps whilst remaining accurate and is therefore the de facto standard for particle movers \cite{boris_good}. The value for equation \ref{eq:vnew} is then substituted into equation \ref{eq:new_pos} to obtain a new position for each particle.


%\be 
%v_{t+1/2} = v_{t- 1/2} + \frac{qE_t}{m} \Delta t
%\ee
%\be 
%x_{t+1} = x_{t} + v_{t+1/2} \Delta t
%\ee 


% There are many ways in which these equations can be integrated, the methods vary in their complexity to implement as well as run time and accuracy. 
%
%
% Depending on the needs of the simulator, a trade off may need to be made between fast run time and high accuracy. 

%%The simplest method of integration is Euler's method.
%%The finite difference method is used again to discretise the equations. 
%\be 
%v_{t+\Delta t} = v_t + \frac{dv}{dt} \Delta t + \frac{d^2v}{dt^2} {(\Delta t)}^2 + ...
%\ee
%\be 
%x_{t+\Delta t} = x_t + \frac{dx}{dt} \Delta t + \frac{d^2x}{dt^2} {(\Delta t)}^2 + ...
%\ee
%
%In the Euler method The largest term neglected is $\propto {(\Delta t)}^2$. After taking $N$ steps where $N = \frac{total time}{\Delta t}$ the error is now proportional to $\Delta t$. For the case of charged particles moving in an electric field the equations of motion can now be written as
%\be
%v_{t+1} = v_t + \frac{qE}{m} \Delta t
%\label{eq:accelerate}
%\ee
%\be 
%x_{t+1} = x_t + v_t \Delta t
%\label{eq:move}
%\ee 
%The field has been solved already in the previous step of the PIC cycle so $E$ is known. This is then used to accelerate the particle to a new velocity \eqref{eq:accelerate}. The new velocity is then used to find a new position \eqref{eq:move}. All particles are given an initial position and velocity at the start of the simulation so $x_0$ and $v_0$ are known.  The Euler method is very simple to implement and fast to run but is only first order accurate.
%
%To demonstrate the stability of the Euler method it will now be applied to the case of a mass on a spring with equation of motion 
%\be 
%\frac{d^2 x}{dt^2} = -\frac{k}{m} x 
%\ee
%where $k$ is the Stiffness constant of the spring and $m$ is the mass of the attached object. An initial value for the position and velocity is required e.g. $v=1, x=0$.  Euler's method would approximate the equations of motion as 
%\be
%v_{t+1} = v_t - \frac{k}{m} x \Delta t 
%\ee
%\be 
%x_{t+1} = x_t + v_t \Delta t
%\ee
%These equations can be solved multiple times to advance the system from its initial state. To check the accuracy of the Euler method the results are compared to the known analytical solution. 
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{Euler}
%\caption{Euler solution to harmonic oscillator compared to analytical solution.}
%\label{fig:E}
%\end{figure}
%The Euler method is unstable and quickly diverges from the analytical solution. This is no surprise based on the simplicity of the method. A more stable, higher order accurate integrator is desired for use in PIC codes.  Euler's method is described as an Explicit method as it uses the state of the system at the current time step to advance the system to its next time state. Implicit methods on the other hand use both the state of the system at the current time and the state of the system at the next time step to evolve the state of the system. An example of this is the Backwards Euler method. In this case the value of the velocity at time $t$ can be expressed as 
%\be
%v_t = v_{t+1 -\Delta t} = v_{t+1} - \Delta t \frac{dy}{dt}\Bigr|_{\substack{t=t+1}}
%\ee
%using a backwards Taylor expansion. Rearranging gives 
%\be 
%v_{t+1} = v_t +  \Delta t \frac{dy}{dt}\Bigr|_{\substack{t=t+1}} 
%\ee 
%A similar expression is found by replacing $v$ with $x$. Applying this to the case of the simple harmonic oscillator 
%\be 
%v_{t+1} = v_t - \Delta t\frac{k}{m} x_{t+1} 
%\ee 
%\be 
%x_{t+1} = x_t - \Delta t\frac{k}{m} v_{t+1} 
%\ee
%The new value of the variable no longer depends on two known values as in the case of the Euler method but now depends on an unknown value from the next time step. It can be solved using the Newton-Raphson method. Implicit schemes are stable but more computationally expensive, this can be offset by the fact they allow for larger time steps to be taken and so can evolve a system to a desired time using less time steps than an explicit scheme. The stability of the numerical methods is not the only thing that limits the size of the time step in PIC codes, the size of the timestep is limited by the natural frequencies of the system \cite{Hockney1981}. This will be discussed more in the practical considerations section. The computational expensive and the need to use small time steps often rules out the use of implicit methods for the particle mover of PIC codes. 
%
%
%Fortunately a more stable, explicit  method exists that is very similar to the Euler method and requires the same number of computations per time step but also has the advantage of being second order accurate. The Leapfrog method expands upon the Euler method. Instead of using the velocity at the beginning of the time step to advance the position it uses an average value of velocity throughout the time step. The leap-frog method involves offsetting the velocity by half a time step from the position. So the velocity of the particles is only known at half integer time steps while the positions are known at integer time steps. 
%\be 
%v_{t+1/2} = v_{t- 1/2} + \frac{qE_t}{m} \Delta t
%\ee
%\be 
%x_{t+1} = x_{t} + v_{t+1/2} \Delta t
%\ee 
%This requires the initial velocities of the particles to be moved back half a time step at the beginning of the simulation, a "de-acceleration", in order to have time centred velocities. This just requires calculating the fields as before. This method is known as Leap-frog because in order to calculate new positions requires a leap over the known velocity. 
%
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{Leapfrog}
%\caption{A graphical representation of the Leap-Frog scheme\cite{shape}}
%\label{fig:Leapfrog}
%\end{figure}
%Instead of using the velocity at time $t$ to move the particle from $t$ to $t+1$ like the Euler method, Leap frog uses the velocity at time $t+\frac{1}{2}$ i.e. the average velocity of the particle between those two times. This method is more accurate than Euler's method for the same computational expense. The only extra step involves pushing back the particles half a time step but this only needs to be carried out once. For this reason Leap-frog is always the preferred choice over Euler and it can be proven to be second order accurate \cite{second_order}.
%This method can also be applied to the case of the mass on a spring as before and compared to the analytical solution.
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{leapfrog}
%\caption{A comparison of the numerical solution to the problem of a mass on a spring using the leapfrog method compared to the analytical solution.}
%\end{figure}
%This was run with as many time steps as Euler. It is clearly more stable. As with all explicit methods there exists a critical time step size, exceeding this leads to numerical instabilities. In the case of leapfrog $w dt <2 $ where $w$ is the highest frequency in the problem, usually the electron plasma frequency. COULD PROVE THIS WITH THE HARMONIC OSCILLATOR. However as mentioned before there are other factors which force the time step to be small for PIC codes so this is not a disadvantage to the Leap Frog method.

%\subsubsection{Yee grid}
\subsubsection{Stability Conditions}
%explicit methods have regirous time constraints for stability condtions. THEN IMPLICIT METHODS EXIST that aim to increase the length of the time step while remaining stable e.g. cite the thesis I'm reading. Then mention why these aren't used.

PIC simulations utilise finite difference equations to find solutions to continuous differential equations on a discretised grid. The use of finite difference equations can provide accurate physical results provided certain stability conditions are met. To ensure the stability of the leap-frog particle advancing algorithm the time step ($\Delta t$) must be sufficiently small such that
\begin{equation}
\omega_p \Delta t <2.
\end{equation}
where $\omega_p $ is the plasma frequency. A time step of this size ensures stability of the algorithm but a further restraint must be placed upon the time step for it to provide accurate results \cite{bible}. 
\begin{equation}
\omega_p \Delta t <0.2.
\end{equation}
The second constraint determines the minimum grid spacing ($\Delta x$) that can be used. 
\begin{equation}
\Delta x \leq \lambda_D
\end{equation}
where $\lambda_D$ is the Debye length. PIC codes can only resolve phenomena that are larger than $\Delta x$, anything smaller than this is smoothed over. PIC codes must resolve the Debye length in order to accurately capture the shielding effects. If this criteria is not met, non-physical numerical heating of the electrons will occur. The temperature of the electrons will increase until the Debye length is such that the stability criteria is met.  
The time step is further constrained by the criteria that no particle should be able to travel more than one grid cell in a given time step known as the Courant-Friedrich-Lewy condition \cite{JH-thesis}.
\be 
\Delta t < \frac{\Delta x}{v_{max}}
\ee
where $v_{max}$ is the speed of the fastest moving particle in the system. For high density, low temperature plasmas $\lambda_D \approx 10^{-6}$ while electron velocities can exceed $10^6 ms^{-1}$. This means thousands of grid cells may be necessary to simulate the $10mm^2$ tip of a Langmuir probe with time steps as small as $10^{-12}s$. Simulations with these demands can only be carried out on supercomputers.
 
 %The first places a constraint on the size of each grid cell ($\Delta x$). 
%\be
%\Delta x < \frac{\lambda_D}{0.2}
%\ee
%PIC codes can only resolve phenomena that are larger than $\Delta x$, anything smaller than this is smoothed over. PIC codes must resolve the Debye length in order to accurately capture the shielding effects. The second constraint is that the plasma frequency must be resolved meaning that
%\be
%\Delta t < \frac{2}{\omega_p}
%\ee


%"Despite having many advantages, the PIC model also has a number of weaknesses. Perhaps
%the most quickly encountered of these is computational efficiency. This is a consequence of
%the statistical model in which numerical fluctuations converge as N−1/2 for N particles; the
%PIC scheme and some modifications can reduce the constant but not the scaling. An associated
%problem is the difficulty in resolving the tail of the distribution, which is often poorly populated
%by statistical methods. It is also challenging to model large ranges of timescales, as short
%timescales require small time steps while long timescales require running many time steps.
%Similarly, large ranges of space scales present similar difficulties for the mesh size. Finally,
%the PIC method requires significant memory and processor resources, and for the foreseeable
%future this will remain the case. Fortunately, the boundaries of what is possible are always
%advancing by virtue of Moore’s law and new algorithms"
%
%"Strict requirements on the spatio-temporal spacing in explicit PIC scheme put constraints on the time
%scales that this method can successfully address. On the other hand it is a very powerful method which
%easily allows for including many additional physical phenomena, such as collisions, production and loss
%of particles, or interaction with solid surfaces."

%\subsubsection{Consequences of Using a Grid}
%\subsubsection{Finite Sized Particles}
%This gives the particle an effective shape, in this case rectangular. The particle also has a finite size of width $\Delta x$. Finite sized particles are a direct result of weighting particles on to the grid. The weighting method determines the effective size and shape of the particle as viewed by grid. Due to the large jumps to grid values caused by the movement of particles from one grid point to the next, the NGP weighting method is a large source of noise in the density and field calculations.
%The weighting puts the fraction of the clouds charge which is in the $J^{th}$ cell to the $X_J$ grid point and the rest of it goes to the $X_{J+1}$ grid point. This  gives the particle a triangular shape, so the particle is effectively a triangular cloud of uniform charge centred at $x_i$ with a width of $2\Delta x$ as it is able to influence a grid point from both sides of it.
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{particleshape}
%\caption{The effective shape of a particle at position x as seen by the grid.\cite{shape}}
%\label{fig:shape}
%\end{figure}

\subsection{Langmuir Probe Simulations}
\subsubsection{Particle Loading and Injection}
In Langmuir probe simulations, and any other plasma discharge simulation, particles are lost to the various collecting surfaces and must be replaced so that a constant density plasma can be simulated. It is therefore necessary to include an additional step  into the PIC cycle in which new particles are introduced into the simulation so that the simulation can converge to a steady state. This step is referred to as particle injection. For clarity, particle loading only occurs at the very beginning of the simulation to distribute plasma throughout the domain, after the initial particles are loaded into the simulation the loading algorithm will not be used again. Particle injection takes place continuously throughout the simulation at the end of the PIC cycle once all other existing particles have been moved.
It is desirable to inject particles at a rate that conserves the plasma density specified at the beginning of the simulation however it is not a necessity. The simulation will reach a steady-state density once the particle injection rate is balanced by the outflow rate of particles. The rate required to maintain a constant density plasma can be estimated as 
\be
R_{constant-density} = v_{th,s}\Delta T * N / \Delta x
\label{eq:rate}
\ee
where $v_{th,s}$ is the thermal velocity of the species, $\Delta T$ the size of the time step, $N$ the number of particles per cell specified at the beginning of the simulation and $\Delta x$ the gridspacing. $R_{constant-density}$ gives the number of particles that must be injected at the beginning of each time step in order to preserve the specified plasma density. The rate will be different for electrons and ions as electrons move across grid cells in much less time due to their low mass, leading to a higher thermal velocity.

Langmuir probe theory is based on the assumption that the electrons and ions have a Maxwellian velocity distribution. Multiple algorithms exist to generate Maxwellian velocity distributions  and these are detailed in Chapter 16 of Birdsall \cite{bible}. A velocity distribution generated with such an algorithm is shown in figure \ref{fig:maxwellian_initial}. The simulation will be required to run for many thousands of time steps in order to reach a steady state solution. For Langmuir probe simulations the requirements of an effective particle injection algorithm are to ensure the probe samples a constant temperature and density plasma as a Langmuir probe in a real plasma would. The algorithm must preserve the specified particle density and conserve the Maxwellian velocity distribution of particles. The former requirement is simply met by injecting particles at the rate at which they move across grid cells as given by equation \ref{eq:rate}. The latter requirement is not so easily met. PIC simulations model a small region of the plasma and use superparticles to sample the velocity distribution. As there are orders of magnitude differences between the number of superparticles followed and the true number of particles in a real plasma, losses of superparticles in a PIC simulation can dramatically change the velocity distribution over very short time scales. The problem is often enhanced by the lack of collisions in fusion-relevant PIC simulations. Due to the high densities and low temperatures of a SOL plasma, PIC simulations can only feasibly model small regions of the plasma. The dimensions of the simulation region are often smaller than any collisional mean free paths, so particles are able to move across the whole domain without experiencing a collision. Collisions drive particles towards Maxwellian distributions. Without this restoring force it is  of crucial importance to sample from the correct velocity distribution during particle injection. 
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{maxwellian_initial}
\caption{The velocity distribution of particles generated by the Maxwellian source function.}
\label{fig:maxwellian_initial}
\end{figure}
In early simulations, particle loading at the beginning of the simulation and particle injection throughout the simulation both sampled the same source function. For every particle a random velocity was chosen sampled from a Maxwellian distribution. However it was found in practise that this did not conserve the Maxwellian distribution. With increasing simulation time, the velocity distribution of the particles narrowed, resulting in an artificial 'cooling' of the plasma. The initial velocity distribution of particles and the distribution after many time steps is shown in figure \ref{fig:EmVsMax}. Although not anticipated the reasons for this observation are simple. Particles are initially loaded into the simulation domain with a range of velocities all sampled from a Maxwellian distribution. Provided there are enough superparticles in the domain, the distribution will be sufficiently represented by the finite number of particles. As time advances, particles exit the simulation once they reach an absorbing boundary layer and new particles enter in the injection phase of the PIC cycle. The fastest particles in the simulation are on average the first to leave as they quickly move across the domain to an absorbing surface. However if the injected particles are also sampled from the same Maxwellian distribution then it is far more likely that the injected particle will have a velocity close to zero rather than a high velocity in the tail of the distribution. As a result the fastest particles leave the simulation quickly to be replaced by slow moving particles that reside in the simulation for a long time. This results in an under representation of the fastest particles in the simulation. The high energy tail disappears, the distribution narrows and the effective temperature of the plasma cools. Ideal probe theory assumes a constant temperature, Maxwellian plasma. Experimental measurements generally work on this assumption too. It is not possible to compare experimental data with simulation results if the simulation plasma is not at a constant temperature. A source function that conserves temperature as the simulation runs is desired. This source function must replace the particles at a rate proportional to their velocity. The fastest particles are required to be replaced more often while the slow moving particles not so much. Rather than the Maxwellian distribution that peaks at $v=0$ the new distribution must fall to zero at this point as those particles should never leave the simulation. A method to investigate the required shape of the source function was established.
\subsubsection{Temperature Conserving Source Function}
A 1D simulation was adequate for the purposes of identifying the correct source function to use. The simulation domain is shown in figure \ref{fig:cloned_domain}. 
%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{cloned_source.eps}
%\caption{A source region on the left-hand side replaces particles that are lost to the wall.}
%\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[height=7cm,width=0.8\textwidth]{cloned_source.pdf}
	\caption{The simulation domain used to determine the temperature conserving source function. Particles born into the source region cannot escape the simulation. Before reflecting off the $x= LS$ plane, source particles are cloned. This clone escapes the source region and replenishes particles lost in the bulk plasma.}
	\label{fig:cloned_domain}
\end{figure}
The model tracks a length of plasma , hundreds of Debye Length ($\lambda_D$) long and follows the motion of all ions and electrons as they move in self-consistent electric fields. The presence of a magnetic field  impacts the source function so to begin with only electrostatic simulations were carried out. On one side of the domain is an absorbing surface that represents the probe. This surface can be held to any potential. Any charged particles that hit the surface are deleted from the simulation and their current recorded. On the opposite side of the domain is a source region of plasma. The purpose of this source region is to supply the bulk plasma with new particles to replenish those lost to the sides. At the beginning of the simulation, a quasi-neutral plasma with a Maxwellian distribution fills the entire domain. Any particles born into the source region ($0\leq x \leq LS$) are trapped in the source region for the duration of the simulation. These source particles travel back and forth in the source region and are reflected at the boundries $x=0$ and $x=LS$. Both edges of the source are held at the same potential ($V_{source}$) which fixes the plasma potential. As there is no potential difference between the two sides and no particles can escape, the plasma source remains at the temperature and density specified at the beginning of the simulation. Both the source plasma and the bulk plasma maintain the Maxwellian distribution throughout the duration of the simulation as shown in figure \ref{fig:maxwell_maintained}.
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{maxwellian_across_domain.pdf}
\caption{The velocity distribution of particles in different regions of the plasma at the end of the simulation. The Maxwellian distribution is conserved.}
\label{fig:maxwell_maintained}
\end{figure}
Any source particle striking the $x=LS$ boundary is copied before being reflected. This copy is identical to the original particle, having the same charge, mass, position and velocity but is not reflected at the source boundary. Instead the copy travels into the bulk plasma, allowing the source region to replenish the bulk plasma. By looking at the velocity distribution of particles exiting the source, it is possible to determine the form of the source function required to conserve a Maxwellian plasma. The distribution of particles leaving the source region is equivalent to the distribution of particles that would exit the simulation of a bulk plasma. It is these velocities that must be sampled in order to maintain a constant temperature plasma. The velocity distribution of particles leaving the source is shown in figure \ref{fig:cloned_distribution}
\begin{figure}[H]
	\centering
	\includegraphics[height=7cm,width=0.8\textwidth]{cloned_distribution.pdf}
	\caption{The velocity distribution of particles that exit the source region.}
\label{fig:cloned_distribution}
	\end{figure}
This is the Emmert source function \cite{Emmert} described by
\be
S_(v) = \frac{ m_i v}{T_s} exp\left(-\frac{m_i v ^2}{2T_s} \right)
\label{eq:Emmert}
\ee
The shape of the source function generated from equation \ref{eq:Emmert} is shown in figure \ref{fig:Emmert}.
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{emmert_source.pdf}
\caption{The Emmert source function only replenishes the high energy tail of the distribution. It is identical to the distribution of particles that escape the reflecting domain in my simulations.}
\label{fig:Emmert}
\end{figure}
As well as providing a source function, this method validated the rate of injection. It was found that the ratio of electrons exiting the source to ions exiting the source was equal to the ratio of their thermal velocities. Now a temperature conserving source function has been determined there is no need for the source region. Simulating the source region requires tracking many particles, which is feasible in 1D simulations but would take up too much computer time for higher dimensions. The simulation domain without the source function is shown in figure  \ref{fig:emmert_domain}.
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{ideal_probe_test.pdf}
\caption{The domain for the 1D probe simulations used to test the source function.}
\label{fig:emmert_domain}
\end{figure}




A plasma source is required to replenish particles that are lost to the probe. This source can be held at any potential, I choose 0V, this sets the plasma potential. The system reaches a steady state over a few ion transit times at which point the current drained by the probe surface reaches a constant value and the plasma density stabilises. 

\subsection{Generating a Velocity from a Source Function}
The Emmert source function is described by equation \ref{eq:Emmert}. The source function is a cumulative distribution function. As this distribution function is invertible it can be used to generate a velocity. The distribution  is in the form of a Weibull distribution 
\be
W(v) = \alpha \beta v^{\beta -1} \exp{-\alpha v^\beta}
\ee 
with $\beta = 2$ and $alpha = \frac{m}{2qT_s}$. 
By applying the fundamental transformation law of probabilities, it is possible to use a randomly generated number $x$, where $0\leq x \leq 1$, from a uniform distribution and transform this into a randomly generated number belonging to the Weibull distribution. 
For two probability distribution functions $p(x)$ and $S(v)$ the fundamental transformation law states
\be 
\left|  {p(x)dx} \right| =\left| d{S(v)dv}  \right|
\ee
or 
\be 
S(v) = p(x) \left|\frac{dx}{dv} \right|
\ee
as $x$ is from a uniform distribution, $p(x)$ is constant and so 
\be 
S(v) = \left| \frac{dx}{dv} \right|
\ee
therefore 
\be
x = \int^v_0 S(v) dv
\ee 
Integrating the Weibull distribution gives 
\be 
x = 1 - \exp{-\alpha v^\beta}
\ee 
Inverting this gives a relation for the velocity in terms of the random number x 
\be
v = \left[-\frac{1}{\alpha}\ln(1-x)\right]^{\frac{1}{\beta}}
\ee 
Substituting in values for $\alpha$ and $\beta$ gives 
\be 
v = \left[\frac{-2qT}{m} \ln(1-x)\right]^{\frac{1}{2}}
\ee
%https://en.wikipedia.org/wiki/Inverse_transform_sampling
%https://www.taygeta.com/random/weibull.html
\subsection{Reproducing Ideal Probe Theory} \label{Section:Ideal}
In order to test the suitability of PIC codes to the study of Langmuir probe behaviour, multiple simulations were carried out to reproduce predictions made from ideal probe theory. The main equations of ideal probe theory will now be summarised before simulation results are presented. A complete description of ideal probe theory can be found in chapter two.
In experiments, a probe IV curve is produced by sweeping the voltage across the probe and measuring the collected current for each voltage. The total current reaching the probe will be the sum of the electron and ion currents to the probe. 
\be
I_{Probe} = I_{ion} + I_{electron} 
\ee
The magnitude of these currents will vary with the applied probe voltage. If the probe is biased such that it is negatively biased with respect to the plasma potential $V_{plasma}$ then ions will be collected at their saturated value and only the portion of the electron population with sufficient energy to overcome the negative bias will be able hit the probe. As the probe bias becomes more negative, less of the electron population can reach the probe. Eventually no electrons reach the probe and the probe only collects the ion saturation current given by 
\be 
I^+_{sat} = n_s e v_B A 
\ee
where $n_s$ is the ion density at the sheath edge, $e$ is the fundamental charge and $A$ the collection area of the exposed probe tip.
If the probe is biased sufficiently positively with respect to $V_{plasma}$ no ions will be able to reach the probe. The current collected by the probe is then the electron saturation current given by 
\be
I^-_{sat} = \frac{1}{4} n_s e \sqrt{\frac{8KT_e}{\pi M_e}}
\ee
Provided the probe bias meets the following criteria
\be
V_{probe} \leq V_{plasma}
\ee 
The current to the probe will consist of the ion saturation current plus a reduced electron current  
\be
I_{probe} = I^+_{sat} + I^-_{sat} \exp\left(\frac{e(V_{probe}-V_{plasma})}{T_e}\right)
\ee
This can be rearranged to simplify the measurement of $T_e$.
\be 
\ln(I_e) = \frac{V_{probe}}{T_e} + \ln(I^+_{sat})
\ee
Taking $V_{plasma} = 0$. By plotting the natural logarithm of the electron current against the probe bias and measuring the gradient it is possible to use probe measurements to determine the electron temperature. 
With a working source function that preserves plasma temperature, simulations should be able to reproduce this important prediction of probe theory. Multiple simulations were run using the domain as shown in figure \ref{fig:emmert_domain}. In each simulation a different bias voltage was applied to the probe, the simulation was run to steady state and the electron and ion current reaching the probe was recorded. The source temperature of the electrons and ions was set to 5eV. Using ideal probe theory allowed the correct temperature to be measured by the simulated probe as shown in figure \ref{fig:ideal_temp}.
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{ideal_theory_emmert.pdf}
\label{fig:ideal_temp}
\caption{The log of the electron current against probe bias voltage for the Emmert source function. The gradient of the natural log plot is equivalent to the reciprocal of the electron temperature. Using the Emmert source function to inject new particles allows the probe to measure the correct source temperature.}
\end{figure}
For comparison the simulations were repeated using the Maxwellian source function rather than the temperature conserving Emmert function to inject particles. As can be seen in figure \ref{fig:e_grad_max}   the probe measures a lower electron temperature, a consequence of the narrowing of the velocity distribution. Figure \ref{fig:EmVsMax} compares the velocity distribution of the electrons at the end of simulation when sampling from each distribution. It is clear that the correct temperature is conserved when sampling from the Emmert source function but a loss of temperature is detected when using the Maxwellian function. For the Maxwellian runs, the probe measured a temperature somewhere in between the original specified temperature and the final plasma temperature.

\begin{figure}[H]
	\centering
	\includegraphics[height=7cm,width=0.8\textwidth]{ideal_theory_maxwell.pdf}
	%\label{fig:e_grad_max}
	\caption{The log of the electron current against probe bias voltage for the Maxwellian source function.The gradient of the natural log plot is equivalent to the reciprocal of the electron temperature. Using the Maxwellian source function to inject new particles results in the probe measuring a temperature that is lower than the specified source temperature.}
	\label{fig:e_grad_max}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=7cm,width=0.8\textwidth]{distributions.pdf}

	\caption{A comparison of the electron velocity distributions. Red- The distribution of electrons at the beginning of both simulations. Green- The electron distribution at the end of the simulation using a Maxwellian source function. Blue- The electron distribution at the end of the simulation using the Emmert source function.}
	\label{fig:EmVsMax}
\end{figure}

The ability to reproduce ideal probe theory confirms the correct choice of source function. This was a crucial starting point. With a reliable source function, scenarios with more complicated physics can now be explored.  


\subsection{Source Function for Magnetised Plasmas}
In order to simulate Langmuir probes in fusion plasmas a magnetic field must be added to the simulations. In the previous simulations, without a magnetic field, it was found that the Emmert source function was correct to use in the x-direction as the rate at which particles were lost to the probe was directly proportional to the $v_x$ component of their velocity. $v_y$ and $v_z$ were sampled from a Maxwellian distribution. The presence of the magnetic field, which does not have to lie along one of the Cartesian axis adds an additional step to the particle injection algorithm. Particle velocities are generated relative to the magnetic coordinates, $v_\parallel , v_{\perp ,1}$ and $v_{\perp ,2}$. These velocities must then be transformed so that they lie along the coordinate axis. In a magnetised plasma the rate at which particles are lost to the wall now depends on their parallel velocity so this velocity should be generated from the Emmert distribution. The two perpendicular velocities are again generated from Maxwellian distributions. The field orientated velocities $v_\parallel , v_{\perp ,1}$ and $v_{\perp ,2}$ are then converted to Cartesian velocities $v_x , v_y , v_z$ by the following transformations \cite{M-Komm}.
\be 
v_x = v_\parallel b_x + v_{\perp ,2} \sqrt{b_y^2 + b_z^2}
\ee
\be
v_y = v_\parallel b_y + \frac{v_{\perp ,1} b_z - v_{\perp ,2} b_x b_y}{\sqrt{b_y^2 + b_z^2}}
\ee
%
%
\be 
v_z = v_\parallel b_z  - \frac{v_{\perp ,1} b_y + v_{\perp ,2} b_x b_z }{\sqrt{b_y^2 + b_z^2}}
\ee
These transformations have been tested to ensure they generate correct parallel velocities for the particles.

\subsection{Floating Wall Conditions}
In simulations of Langmuir probes it is desirable to implement floating surfaces to represent a probe operated in floating mode or a surrounding divertor tile. To include floating boundary conditions in VSim requires the use of heavy particles. These are particles with the same charge as that of an ion or an electron but a mass that is sufficiently large such that the particle will not move due to the forces imparted upon it throughout the duration of the simulation. A mass of 1kg is sufficient. If a particle comes into contact with a floating surface in the simulation, the particle is deleted from the simulation. A heavy particle is then emitted at this point of absorption, so the charge of the absorbed particles build up on the wall. A charge and electrostatic potential naturally build up on the floating surface this way without having to impose a floating potential. To test this boundary condition a simulation was carried out with the simulation domain used in section \ref{Section:Ideal}. Instead of biasing the right hand side of the simulation to a set probe bias potential, the wall now absorbed particles that hit it and re-emitted heavy particles in their place. The simulation was run until the wall reached a constant potential and drained a steady current. The current and potential on the wall agree well with what is expected based on the electron temperature as shown in figure \ref{fig:floating_point1}.
\begin{figure}[H]
\centering
\includegraphics[height=7cm,width=0.8\textwidth]{floating_point.pdf}

\caption{The green circle represents the floating potential and current drained by the probe using the heavy particle floating boundary condition. }
\label{fig:floating_point1}
\end{figure}

\subsection{Assumptions the model makes}
In our simulations there are no collisions between the charged particles. Typical plasma parameters used in the simulations are of the order $n_e \approx 1 \times 10^{18}$ and $T_e \approx 10eV$. The simulation domain usually spans no longer than $5mm$ in either direction. The demand that the grid must resolve the Debye length means it is not feasible to simulate larger lengths than this as the required amount of grid points will be too computationally demanding.    The mean-free path ($\lambda$) for both electrons and ions is calculated to far exceed the length of typical simulation domains  for the plasma parameters. Using equations from Wesson \cite{Wesson} it is calculated that $\lambda_{electron} = 7.2cm$ and $\lambda_{ion} = 10cm$ for the stated plasma parameters.  As a result particles can travel across the entire simulation domain multiple times without experiencing a collision. It has also been assumed that there are no neutrals or impurities present so the plasma consists of electrons and singly charged ions. %Plasma-surface interaction effects such as secondary electron emission and sputtering have also been neglected.

\subsection{Consquences of the Computational Grid} \label{Section:grid}
\subsubsection{Finite Sized Particles}
%A physical plasma consists of many point sized particles moving due to their random thermal motion and the presence of electric and magnetic fields. 

Rather than the point sized particles of a physical plasma, particles in a PIC simulation are finite sized clouds of uniform charge. Finite sized particles are a direct result of weighting particles on to the grid. The weighting method determines the effective size and shape of the particle as viewed by grid. For the first order weighting scheme described above, a fraction of the clouds charge which is in the $J^{th}$ cell is weighted to the $X_J$ grid point and the rest of it goes to the $X_{J+1}$ grid point. This gives the particle a triangular shape, so the particle is effectively a triangular cloud of uniform charge centred at $x_i$ with a width of $2\Delta x$ as it is able to influence grid points either side of it.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{triangle.pdf}
\caption{The effective shape of a particle at position x as seen by the grid. At the earlier time the majority of the particles charge is deposited to the $X_J$ grid point as this is the particles nearest grid point. At a later time the particle has advanced and now deposits most of its charge to grid point $X_{J+1}$.}
\label{fig:shape}
\end{figure} 
 
Typical plasmas are considered weakly coupled systems as they have a large number of particles in a given cube of volume $\lambda_D ^3$. This means the trajectory of each particle is affected by a very large number of particles, so the trajectory is smooth as many particles contribute to the electric field. Effects from close encounters with other particles do not dominate particle motion. A strongly coupled system on the other hand is one in which there are few particles per Debye length. The trajectory of a particle is then strongly affected by a collision with another particle and the electric field is noisy and irregular. PIC codes manage to replicate the physics of real plasmas despite using fewer particles by using finite sized particles. Finite sized particles interact more weakly during close encounters than the point size particles of a real plasma. Once finite size particles begin to overlap, the force they exert on each other decreases, reaching zero once the particles fully overlap \cite{Lapenta}. By reducing the interaction among particles PIC codes can successfully model weakly coupled plasmas. In terms of long range interactions, particles in a PIC code only interact with particles in their own and closely neighbouring grid cells. This is physically justified as the grid cell is on the order of a Debye length. In a physical plasma the field from a localised charge is screened out at distances greater than a Debye length. 
 

\subsubsection{Self Force}
The presence of the spatial grid can introduce non-physical effects into the simulation, an example of which is the self force. The self force is the name given to the phenomenon in which a charged particle becomes aware of its own field and so repels itself. This non-physical force is inherent in any system that uses the spatial grid to work out charge density and electric field values. Imagine a charged particle placed in between two grid points on a one dimensional grid. Let the particle reside closer to the left grid point to begin with. Using the first order weighting scheme the particle will deposit more of its charge on to the left grid point than it will on to the right. This density will then be used to work out a potential and finally a field which is then interpolated back to the particles position, not necessarily using the same weighting scheme but for now consider it is the same. The particle will feel a larger force from the grid point on its left than it will do from the grid point on its right due to the relative distances and as a result it will be repelled from the left grid point and head towards the right. Although this argument assumed a first order weighting scheme, the self force is present with any method of weighting. 
It is possible to prove that the self force is introduced by the addition of a spatial grid rather than from any approximations made in using finite difference methods to solve Poissons equation. The following method is adapted from the textbook 'Numerical particle-in-cell Methods: Theory and Applications' \cite{selfforce}. 

The electric potential due to a point charge with charge $q$ at a distance $r$ is given by 
\be 
\psi = \frac{1}{4 \pi \epsilon_0} \frac{q}{r} = \frac{C q}{r}
\label{exact_potential}
\ee 
where $C$ is a constant. Now introduce a spatial grid with spacing $dx$. Let the particle be a distance $r$ from the grid point on the left and so a distance $dx - r$ from the point on the right. The particle will deposit a charge $\rho_1$ on the left grid point and a charge $\rho_2$ on the right where
\be 
\rho_1 = q\frac{(dx-r)}{dx}
\ee 
and
\be 
\rho_2 = q\frac{r}{dx}
\ee 
The particle will feel a potential that is a combination of the potential from the two grid points. From \eqref{exact_potential}the potential at the particle is 
\be 
\frac{C q}{dx} \left(\frac{dx - r}{r} + \frac{r}{dx - r} \right) 
\ee 
The force at the particle is given by 
\be 
F = -\frac{\partial \psi}{\partial r} q = Cq^2 dx\frac{dx-2r}{r^2(dx-r)^2}
\ee 
Despite the potential and electric field having been solved exactly without finite difference methods there still exists a self force due to the introduction of a spatial grid. This force is zero if the particle is at the midpoint of a cell ($r=\frac{dx}{2}$) and otherwise acts to repel the particle from its closest grid point.

It is possible to quantify this force in the case of a complete PIC simulation where finite difference methods are used. As before Poisson's equation is discretised 
\be 
-\rho_J = \frac{\psi_{J+1} - 2\psi_J + \psi_{J-1}}{(dx)^2}
\ee 
This equation can be solved analytically without a matrix equation.
\be 
\psi_J = \frac{I-J}{I} A + \frac{J}{I} B + \frac{dx^2}{I} \left[(I-J) \sum\limits_{k=1}^J k\rho_k   + J \sum\limits_{k=J+1}^{I-1} (I-k)\rho_k \right]
\label{potential_analyitical}
\ee
Where $I$ is the total number of grid cells , $A$ is the potential at the left hand side boundary and $B$ the potential at the right hand side. 
This will now be carried out for one particle. Let the particle lie between grid points $x_{\alpha -1}$ and $x_\alpha$ at position $x$. The offset ($\sigma$) is given by 
\be 
\sigma = \frac{x - x_{\alpha -1}}{dx}
\ee
Using \eqref{potential_analyitical} it is possible to calculate the potential at the grid points either side of the particle. 
\be
\psi_\alpha  = \frac{I-\alpha}{I}A +\frac{\alpha}{I}B + \frac{dx^2}{I}(I-\alpha) \left[(\alpha-1)\rho_{\alpha-1} + \alpha \rho_{\alpha} \right]
\ee
\be 
\psi_{\alpha -1} = \frac{I-\alpha +1}{I}A + \frac{\alpha - 1}{I} B +\frac{dx^2}{I}(\alpha -1) \left[(I-\alpha +1)\rho_{\alpha -1} + (I-\alpha)\rho_\alpha \right] 
\ee
Similar expressions can be derived for $\psi_{\alpha+1}$ and $\psi_{\alpha-2}$. These can then be used to derive the value of the electric field at the adjacent grid points. 
\be 
E_\alpha = - \frac{\psi_{\alpha+1} - \psi_{\alpha}}{dx} = \frac{A-B}{I dx} - \frac{dx}{I}\left[(1-\alpha) \rho_{\alpha -1} - \alpha \rho_\alpha \right]
\ee 
\be 
E_{\alpha-1} = - \frac{\psi_{\alpha} - \psi_{\alpha-1}}{ dx} = \frac{A-B}{I dx} - \frac{dx}{I} \left[(1-\alpha) \rho_{\alpha -1} +(I- \alpha) \rho_\alpha \right] 
\ee
Using the first order weighting method, the force at the particle $E_i$ will be given by 
\be 
E_i = E_{\alpha -1 } (1-\sigma) + \sigma E_\alpha = \frac{A - B}{I dx} - \frac{dx}{I} \left[(1- \alpha) \rho_{\alpha -1} + (I -\alpha  - I \sigma) \rho_\alpha \right]
\ee
The self force is inherent in PIC codes and always acts to repel a particle from its nearest grid point. Interestingly this equation shows the impact that boundary conditions have on the self force. The force not only depends on the location of the particle in the grid cell but also in which cell it lies in. Provided there are sufficient amount of particles the self force will be negligible.

%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{e_grad.PNG}
%\label{maxwell}
%\caption{The gradient of the natural log plot is equivalent to the reciprocal of the electron temperature.}
%\end{figure}






%For the IOP conference I wanted to produce probe IV curves from my simulations and then apply ideal probe theory to the IV curve to see if I could measure the plasma temperature and density. In essence treating the simulation measurements in the same way as experimental data. Ideal probe theory assumes no magnetic fields, collisions or plasma-surface interactions, so if I set my simulations up without these effects I should be able to reproduce the fundamental prediction of ideal probe theory - the electron current drops off exponentially as the probe is biased more negatively with respect to the plasma potential, this is expressed in equation \eqref{e_falloff}. 
%\be
%\label{e_falloff}
%I_{e} = I_{e,sat} e^{-\frac{V_{probe}}{T_e}}
%\ee
%Where $V_{probe}$ is the bias applied to the probe relative to the plasma, $T_e$ the electron temperature in eV and $I_{e,sat}$ the electron saturation current. This is the electron current the probe would collect if $V_{probe} \geq 0$. This can be rearranged to simplify the measurement of $T_e$.
%\be 
%\ln(I_e) = \frac{V_{probe}}{T_e} + \ln(I_{e,sat})
%\ee
%By plotting the natural logarithm of the electron current against the probe bias and measuring the gradient it is possible to measure the electron temperature. 
%
%
%\subsection{The Simulation Model}
%All results presented here were obtained during the VORPAL trial period using a single core license. The simulations were carried out in 1D. The model tracks a length of plasma , hundreds of Debye Length ($\lambda_D$) long and follows the motion of all ions and electrons as they move in self-consistent electric fields. On one side of the domain is an absorbing surface that represents the probe, any potential can be set to this surface and any charged particles that hit the surface are deleted from the simulation and their current recorded. On the opposite side of the domain is another surface that represents the plasma source. A plasma source is required to replenish particles that are lost to the probe. This source can be held at any potential, I choose 0V, this sets the plasma potential. The system reaches a steady state over a few ion transit times at which point the current drained by the probe surface reaches a constant value and the plasma density stabilises. The domain is shown below. 
%% The simulations however, already have the perfect diagnostic as the position and velocity of all particles is known. This means I can 
% 
%
%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{IOP_model.PNG}
%\caption{A source region on the left-hand side replaces particles that are lost to the wall.}
%\end{figure}
%
%At the start of the simulation the domain is filled with a uniform density, quasi-neutral plasma. The particles are given a velocity randomly generated from a Maxwellian distribution as shown in figure \ref{maxwell}. For a given run the probe will be biased to a potential and then run until steady state. Completing multiple runs at different bias voltages allows a probe IV curve to be constructed. Simulations were only run in the ion collection region as this is the area of interest in fusion experiments. As mentioned at the end of my first year report this model has a problem with temperature conservation due to the injection of particles to replace those lost to the probe. In my initial attempt to replace lost particles I simply injected new particles with velocities again sampled from a Maxwellian distribution. However, the faster a particle is, the quicker it will be lost to the wall. Due to the nature of a Maxwellian distribution, it is far more likely that the newly injected particle will have a low velocity close to zero and so naturally the high energy tail of the distribution is lost over time. This presents a problem when attempting to apply ideal probe theory to my simulation results. $T_e$ is not a constant in my simulations and varies with $V_{probe}$. As a result each run with a certain $V_{probe}$ sits on it's own IV curve and cannot be compared to the other runs. I needed a source function that only replaced the fast particles lost to the wall and not the zero velocity particles that didn't leave the simulation. It was not obvious what that source function should be, however, I did have another method to get around this.
%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{maxwellian.PNG}
%\label{maxwell}
%\caption{The initial velocity distribution of particles in the simulation. A particle's velocity is sampled from a Maxwellian distribution.}
%\end{figure}
%
%\subsection{Improvements to the Model}
%In the improved model I did away with the source function altogether. Half way across the simulation I placed a surface that reflected any particles that hit it coming from the left and absorbed any particles that hit it coming from the right hand side. Particles that were born on the LHS of the simulation were trapped by this reflecting plane and would travel back and forth throughout the simulation. The plasma on the LHS conserved the temperature and density specified at the beginning of the simulation, this didn't change over time. I was then able to turn this reflecting region into a source for my simulations. Just before a particle hit the reflecting plane, a clone was made of the particle with the same velocity. This clone was allowed to pass through the reflecting surface and travel to the probe on the right hand side while the original particle was reflected and travelled back towards the left hand side of the domain. I now had a particle source that only replaced particles that should have been able to escape the domain and so replaced the high energy tail lost to the wall. I was able to use this reflecting and cloning source method to reproduce ideal probe theory.
%
%\subsection{Results}
%By recording the electron current drained by the probe in steady state for different probe biases I was able to produce the following graph. 
%
%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{e_grad.PNG}
%\label{maxwell}
%\caption{The gradient of the natural log plot is equivalent to the reciprocal of the electron temperature.}
%\end{figure}
%
%By measuring the gradient I was able to determine the temperature of the source plasma in the reflecting region. This was repeated for different initial temperatures and always determined the correct result. 
%
%I was also able to produce a probe IV curve in the ion collection regime which agreed well with the theoretical prediction of equation \ref{theoretical}. 
%
%\be
%\label{theoretical} 
%I_{total} = I_{sat} (1 - e^ {(V_{probe} - V_{float})/T_e})
%\ee
%\begin{figure}[H]
%\centering
%\includegraphics[height=7cm,width=0.8\textwidth]{IV_curve.PNG}
%\label{maxwell}
%\caption{Plot of the probe IV curve in the ion collection region compared to the theoretical prediction. }
%\end{figure}
%
%The ability to reproduce ideal probe theory was a crucial starting point before going on to investigate more realistic scenarios. 





\section{References}
\bibliography{references}

\end{document}